# Evaluating-State-of-the-Art-Transformer-Models-for-Efficient-Text-Summarization

This study explores the performance of state-of-the-art text summarization models, including
T5-Small, T5-Base, BART, Pegasus, and MT5-Base, in generating accurate and concise
summaries from social media and social network platform texts. The models, based on the
transformer architecture, are evaluated using various metrics such as ROUGE, BLEU, and
BERTScore to measure the quality, coherence, and relevance of the generated summaries.
Additionally, the computational efficiency of each model is analyzed through execution time
measurements. Results indicate that T5-Small and T5-Base perform well in summary quality,
with T5-Base generally surpassing T5-Small due to its larger size. BART and Pegasus
demonstrate superior capabilities in producing coherent summaries, with BART excelling in
noisy data and Pegasus in conciseness. MT5-Base shows strong performance on multilingual
datasets. T5-Small stands out for its computational efficiency, making it suitable for real-time
applications. This research highlights the importance of selecting the appropriate
summarization model based on task-specific needs and resource constraints, providing
valuable insights for future advancements in tackling information overload in various domains.
